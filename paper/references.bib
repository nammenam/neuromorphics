@article{Placeholder,
  author  = {?},
  title   = {Placeholder},
  year    = {2025},
  notes   = {this is a placeholder reference}
}
%  -------------------------------------------------------------------
% --- ARTIFICIAL INTELLIGENCE (Foundations)
% -------------------------------------------------------------------

% Summary: This is the foundational paper of Artificial Intelligence.
% Alan Turing proposes the "Imitation Game" (now the Turing Test)
% to answer the question, "Can machines think?" It sets the entire
% philosophical and practical agenda for the field.
@article{Turing1950,
  author  = {A. M. Turing},
  title   = {Computing Machinery and Intelligence},
  journal = {Mind},
  volume  = {LIX},
  number  = {236},
  pages   = {433--460},
  year    = {1950}
}

% Summary: This paper presents the first mathematical model of a
% "neuron" (the McCulloch-Pitts neuron). The authors show how these
% simple binary units can be networked together to perform any logical
% function, laying the groundwork for all future neural networks.
@article{McCulloch1943,
  author  = {Warren S. McCulloch and Walter Pitts},
  title   = {A logical calculus of the ideas immanent in nervous activity},
  journal = {The Bulletin of Mathematical Biophysics},
  volume  = {5},
  number  = {4},
  pages   = {115--133},
  year    = {1943}
}

% Summary: This paper introduces the "Perceptron," the first algorithm
% for a trainable neural network. It's a linear classifier that can
% learn from data, and it's the direct ancestor of modern,
% multi-layered networks.
@article{Rosenblatt1958,
  author  = {Frank Rosenblatt},
  title   = {The perceptron: a probabilistic model for information storage and organization in the brain},
  journal = {Psychological Review},
  volume  = {65},
  number  = {6},
  pages   = {386--408},
  year    = {1958}
}

% Summary: This is the paper that (re)popularized the backpropagation
% algorithm. It provided an efficient way to train deep, multi-layer
% neural networks, overcoming the limitations of the original
% Perceptron. This algorithm is the engine behind the entire
% modern deep learning revolution.
@article{Rumelhart1986,
  author  = {David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
  title   = {Learning representations by back-propagating errors},
  journal = {Nature},
  volume  = {323},
  pages   = {533--536},
  year    = {1986}
}

% Summary: This is the "AlexNet" paper. By using a deep convolutional
% neural network (CNN) trained with backpropagation on GPUs, the
% authors won the 2012 ImageNet competition by a massive margin.
% This single paper ignited the modern boom in deep learning.
@inproceedings{Krizhevsky2012,
  author    = {Alex Krizhevsky and Ilya Sutskever and Geoffrey E. Hinton},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems 25 (NIPS 2012)},
  pages     = {1097--1105},
  year      = {2012}
}

% -------------------------------------------------------------------
% --- NEUROSCIENCE (Foundations)
% -------------------------------------------------------------------

% Summary: This is arguably the most important paper in neuroscience.
% Hodgkin and Huxley used the squid giant axon to develop a complete
% mathematical model of the action potential (the "spike").
% It describes how ion channels (Na+ and K+) create the nerve impulse.
@article{Hodgkin1952,
  author  = {Alan L. Hodgkin and Andrew F. Huxley},
  title   = {A quantitative description of membrane current and its application to conduction and excitation in nerve},
  journal = {The Journal of Physiology},
  volume  = {117},
  number  = {4},
  pages   = {500--544},
  year    = {1952}
}

% Summary: This is not a paper, but the foundational book that
% proposed the mechanism for synaptic plasticity now known as
% "Hebbian Learning." It contains the famous phrase:
% "Neurons that fire together, wire together." This is the
% biological basis for learning and memory that inspires many AI rules.
@book{Hebb1949,
  author    = {D. O. Hebb},
  title     = {The Organization of Behavior: A Neuropsychological Theory},
  publisher = {Wiley & Sons},
  address   = {New York},
  year      = {1949}
}

% Summary: This Nobel Prize-winning work is a cornerstone of
% sensory neuroscience. Hubel and Wiesel recorded from neurons in the
% cat's visual cortex and discovered its hierarchical structure,
% finding "simple" and "complex" cells that respond to specific
% orientations and edges. This work directly inspired the
% architecture of modern Convolutional Neural Networks (CNNs).
@article{Hubel1962,
  author  = {David H. Hubel and Torsten N. Wiesel},
  title   = {Receptive fields, binocular interaction and functional architecture in the cat's visual cortex},
  journal = {The Journal of Physiology},
  volume  = {160},
  number  = {1},
  pages   = {106--154},
  year    = {1962}
}

% -------------------------------------------------------------------
% --- NEUROMORPHIC COMPUTING (Foundations)
% -------------------------------------------------------------------

% Summary: This paper, by Carver Mead, essentially founded the field
% of neuromorphic engineering. Mead argued for building electronic
% circuits that operate in the same way as biological neurons
% (using analog, sub-threshold physics) rather than just simulating
% them on digital computers.
@article{Mead1990,
  author  = {Carver Mead},
  title   = {Neuromorphic electronic systems},
  journal = {Proceedings of the IEEE},
  volume  = {78},
  number  = {10},
  pages   = {1629--1636},
  year    = {1990}
}

% Summary: This is a classic early example of Carver Mead's vision
% in practice. Mahowald and Douglas built a "silicon neuron"â€”a
% physical VLSI chip that implemented the Hodgkin-Huxley model's
% dynamics, successfully mimicking the electrical behavior of a
% real biological neuron in hardware.
@article{Mahowald1991,
  author  = {Misha Mahowald and Rodney Douglas},
  title   = {A silicon neuron},
  journal = {Nature},
  volume  = {354},
  pages   = {515--518},
  year    = {1991}
}

% Summary: This paper describes the SpiNNaker (Spiking Neural Network
% Architecture) project. It's a landmark in large-scale neuromorphic
% systems, using a million ARM processors in parallel to simulate
% spiking neural networks in real-time, focusing on computational
% neuroscience and robotics.
@article{Furber2014,
  author  = {Steve B. Furber and Francesco Galluppi and Steve Temple and Luis A. Plana},
  title   = {The SpiNNaker Project},
  journal = {Proceedings of the IEEE},
  volume  = {102},
  number  = {5},
  pages   = {652--665},
  year    = {2014}
}
